# =============================================================================
# CEI-ToM DMLR 2026 Configuration
# =============================================================================

llm_inference:
  # Model sets by execution mode
  models:
    # TEST mode: single model for quick validation
    test:
      - id: "gpt-5-mini"
        provider: "openai"

    # COMPLETE mode: baseline models
    complete:
      # Commercial API models
      - id: "gpt-5-mini"
        provider: "openai"
      - id: "claude-sonnet-4-5"
        provider: "anthropic"
      - id: "grok-4-1-fast-non-reasoning"
        provider: "xai"
      - id: "gemini-2.5-flash"
        provider: "google"
      # Open-source models via API providers
      - id: "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
        provider: "together"
      - id: "accounts/fireworks/models/deepseek-v3p1"
        provider: "fireworks"
      - id: "Qwen/Qwen2.5-7B-Instruct-Turbo"
        provider: "together"

# =============================================================================
# PRICING (USD per 1M tokens, as of 2026-02-07)
# =============================================================================
pricing_usd_per_1m_tokens:
  openai:
    gpt-5-mini:           {input: 0.25,  output:  2.00}
    _default:             {input: 0.25,  output:  2.00}
  anthropic:
    claude-sonnet-4-5:    {input: 3.00,  output: 15.00}
    _default:             {input: 3.00,  output: 15.00}
  google:
    gemini-2.0-flash:     {input: 0.10,  output:  0.40}
    gemini-2.5-flash:     {input: 0.30,  output:  2.50}
    _default:             {input: 0.30,  output:  2.50}
  xai:
    grok-4-1-fast-non-reasoning: {input: 0.20, output: 0.50}
    _default:             {input: 0.20,  output:  0.50}
  fireworks:
    deepseek-v3p1:        {input: 0.15,  output:  0.75}
    _default:             {input: 1.00,  output:  5.00}
  together:
    meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo: {input: 0.88, output: 0.88}
    Qwen/Qwen2.5-7B-Instruct-Turbo: {input: 0.30, output: 0.30}
    _default:             {input: 0.15,  output:  0.60}
  ollama:
    _default:             {input: 0.0,   output:  0.0}
