# Reproducibility Checklist for CEI Benchmark

*Following the [ML Reproducibility Checklist](https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf)*

## Dataset Documentation

| Item | Status | Location |
|------|--------|----------|
| Datasheet completed | Yes | [DATASHEET.md](DATASHEET.md) |
| Data format documented | Yes | [README.md](README.md) |
| Preprocessing steps documented | Yes | [README.md](README.md) |
| Data splits defined | Yes | Generated by pipeline (`create_splits` stage) |

## Code Availability

| Item | Status | Location |
|------|--------|----------|
| Code publicly available | Yes | This repository |
| Dependencies specified | Yes | [pyproject.toml](pyproject.toml) |
| Installation instructions | Yes | [README.md](README.md) |
| Pipeline documented | Yes | [scripts/run_pipeline_dmlr2026.py](scripts/run_pipeline_dmlr2026.py) |

## Experimental Details

| Item | Status | Location |
|------|--------|----------|
| Prompt templates provided | Yes | [prompts/](prompts/) |
| Evaluation metrics defined | Yes | [README.md](README.md); [prompts/README.md](prompts/README.md) |
| Ground truth derivation explained | Yes | Paper Section 5.3 |

## Statistical Reporting

| Item | Status | Location |
|------|--------|----------|
| Confidence intervals reported | Yes | Pipeline output (`reports/dmlr2026/`) |
| Bootstrap methodology documented | Yes | Paper Section 4.3 |
| Inter-annotator agreement reported | Yes | Paper Table 3 |

## Annotation Process

| Item | Status | Location |
|------|--------|----------|
| Annotator qualifications documented | Yes | Paper Section 4.1 |
| Training protocol described | Yes | Paper Section 4.1 |
| Annotation guidelines provided | Yes | Paper Appendix B |
| Quality control pipeline documented | Yes | Paper Section 4.3 |

## Verification Commands

Run these commands to verify the dataset and reproduce all analyses:

```bash
# 1. Install dependencies
uv pip install -e ".[figures]"

# 2. Run full analysis pipeline (local stages only, no API calls)
python scripts/run_pipeline_dmlr2026.py --stage all_local

# 3. Generate paper figures
python scripts/generate_figures.py

# 4. Verify outputs
ls reports/dmlr2026/
```

## Expected Outputs

After running the pipeline, `reports/dmlr2026/` should contain:

| File | Description |
|------|-------------|
| `agreement_results.json` | Fleiss' kappa with bootstrap CIs per subtype |
| `power_distribution.json` | Power relation counts |
| `human_performance.json` | Unanimous/majority/split agreement patterns |
| `vad_analysis.json` | ICC(2,1) for VAD dimensions |
| `splits.json` | Stratified train/val/test splits |
| `worked_examples.json` | Candidate worked examples |

After running figure generation, `reports/dmlr2026/figures/` should contain:

| File | Description |
|------|-------------|
| `fig1_confusion_matrix.pdf` | 8x8 emotion confusion heatmap |
| `fig2_difficulty_distribution.pdf` | Agreement difficulty by subtype |
| `fig3_linguistic_analysis.pdf` | Text length vs agreement |
| `fig4_agreement_by_subtype.pdf` | Agreement rates per subtype |

## Paper Claims Verification

| Claim | Verification Command |
|-------|---------------------|
| 300 scenarios | `python scripts/run_pipeline_dmlr2026.py --stage all_local` (check log) |
| 5 pragmatic subtypes | Verify 5 CSV files in `data/human-gold/` |
| 3 annotators per subtype | Check annotator columns in CSVs |
| Inter-annotator agreement | Pipeline `verify_agreement` stage |
| Power distributions | Pipeline `verify_power` stage |

## Environment

Tested on:
- Python 3.10, 3.11, 3.12
- macOS, Linux
- Dependencies as specified in pyproject.toml
