\documentclass[twoside,11pt]{article}

% DMLR style file
\usepackage{dmlr2e}

% Additional packages
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{array}
\usepackage{subcaption}
\usepackage{enumitem}

% Custom commands
\newcommand{\cei}{\textsc{CEI}}
% \kappa is already defined in LaTeX

% Heading arguments: {volume}{year}{pages}{submitted}{published}{paper id}{authors}
\dmlrheading{1}{2026}{1-\pageref{LastPage}}{2/26}{TBD}{TBD-0000}{Anonymous}
\def\openreview{\url{https://openreview.net/forum?id=XXXX}}

\ShortHeadings{CEI Benchmark: Pragmatic Reasoning Dataset}{Anonymous}
\firstpageno{1}

\begin{document}

\title{CEI: A Benchmark for Evaluating Pragmatic Reasoning in Language Models}

\author{\name Anonymous Authors}

\editor{TBD}

\maketitle

\begin{abstract}
Pragmatic reasoning---inferring intended meaning beyond literal semantics---is fundamental to human communication yet remains challenging for large language models. We introduce the Contextual Emotional Inference (CEI) Benchmark, a dataset of 300 human-validated scenarios for evaluating pragmatic disambiguation in LLMs. Each scenario consists of a situational context, speaker-listener roles with explicit power relations, and an ambiguous utterance requiring pragmatic interpretation. The dataset spans five pragmatic subtypes (sarcasm/irony, mixed signals, strategic politeness, passive aggression, deflection/misdirection), four social domains (workplace, family, friendship, service), and three power relations (peer, higher$\rightarrow$lower, lower$\rightarrow$higher). All scenarios were independently labeled by three trained annotators. Inter-annotator agreement (Fleiss' $\kappa = 0.12$--$0.25$ by subtype) confirms that pragmatic inference admits multiple valid interpretations, validating both the task difficulty and the need for multi-annotator collection. We document our annotation methodology, including a novel 4-level quality control pipeline integrating automated statistical checks with expert human adjudication. CEI is released under CC-BY-4.0 to support research on pragmatic AI evaluation.
\end{abstract}

\begin{keywords}
pragmatic reasoning, benchmark, language models, annotation, social reasoning, emotion inference
\end{keywords}

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

When a junior employee tells their manager ``Sure, I'll handle the extra work this weekend,'' the intended meaning depends critically on context: Is this sincere agreement? Strategic politeness masking reluctance? Passive-aggressive compliance? Pragmatic reasoning---the inference of intended meaning from what is literally said---is fundamental to human communication \citep{grice1975logic,sperber1986relevance}.

Large language models are increasingly deployed for communication analysis tasks including HR screening, sentiment analysis, and content moderation. Yet these applications require pragmatic interpretation that integrates contextual cues, social roles, and power dynamics. Existing benchmarks focus on narrow pragmatic phenomena such as sarcasm detection \citep{ghosh2020semeval} or test different capabilities entirely \citep{sap2019socialiqa,le2019revisiting}. No comprehensive benchmark exists for single-turn pragmatic disambiguation across diverse social contexts.

This paper introduces the \textbf{Contextual Emotional Inference (CEI) Benchmark}, a dataset of 300 human-validated scenarios spanning 5 pragmatic subtypes, 4 social domains, and 3 power relations. Each scenario includes 3 independent annotations (900 total), enabling analysis of both model performance and human disagreement patterns. We document inter-annotator agreement (Fleiss' $\kappa = 0.12$--$0.25$ by subtype), which reflects the inherent difficulty of pragmatic interpretation rather than annotation noise. To ensure data quality, we developed a 4-level automated QA pipeline integrating statistical consistency checks, agreement analysis, semantic plausibility validation, and expert adjudication. The full dataset, annotation guidelines, and QA code are released under permissive licenses (CC-BY-4.0 for data, MIT for code).

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

\subsection{Pragmatic Reasoning in NLP}

Pragmatic reasoning has deep theoretical foundations in Gricean maxims \citep{grice1975logic}, relevance theory \citep{sperber1986relevance}, and the Rational Speech Act framework \citep{goodman2016pragmatic}. Computational approaches have addressed implicature resolution \citep{jeretic2020natural}, pragmatic inference in dialogue \citep{andreas2016reasoning}, and figurative language understanding \citep{joshi2017automatic}.

\subsection{Existing Benchmarks}

Prior benchmarks address related but distinct phenomena. In sarcasm detection, SARC \citep{khodak2018large} provides 1M self-labeled Reddit comments while iSarcasm \citep{oprea2020isarcasm} offers 4.5K instances with author-intended labels. Recent evaluations find GPT-4 achieves $\sim$75\% accuracy on standard sarcasm benchmarks but drops to $\sim$60\% on subtle contextual cases \citep{liu2024sarcasm}. For social reasoning more broadly, SocialIQa \citep{sap2019socialiqa} tests commonsense reasoning about social situations, while ToMi \citep{le2019revisiting} and FANToM \citep{kim2023fantom} evaluate theory of mind in narrative contexts.

Work on politeness and indirect speech provides another relevant thread. The Stanford Politeness Corpus \citep{danescu2013computational} annotates politeness in online requests, and recent probing studies \citep{fu2023politeness} find that models recognize politeness markers but fail to infer underlying intent when polite language masks disagreement.

CEI addresses a gap left by these efforts: no existing benchmark combines multiple pragmatic subtypes, explicit power relations, multi-annotator validation with documented agreement, and contextual emotional inference as the evaluation target.

%==============================================================================
\section{Dataset Design}
\label{sec:design}
%==============================================================================

\subsection{Scenario Structure}

Each CEI scenario comprises four components:
\begin{enumerate}[leftmargin=*,noitemsep]
    \item \textbf{Context:} A situational description (2--4 sentences) establishing setting and background.
    \item \textbf{Speaker/Listener Roles:} Explicit relationship labels encoding power dynamics.
    \item \textbf{Utterance:} What the speaker says---the pragmatically ambiguous target.
    \item \textbf{Annotation Target:} The speaker's likely emotional state and its VAD (Valence-Arousal-Dominance) profile.
\end{enumerate}

\subsection{Pragmatic Subtypes}

Table~\ref{tab:subtypes} defines the five pragmatic subtypes, selected based on established research traditions.

\begin{table}[t]
\caption{CEI Pragmatic Subtypes with Definitions and Examples}
\label{tab:subtypes}
\centering
\small
\begin{tabular}{@{}p{2.2cm}p{4.5cm}p{5.5cm}@{}}
\toprule
\textbf{Subtype} & \textbf{Definition} & \textbf{Example Utterance} \\
\midrule
Sarcasm/Irony & Saying the opposite of what is meant, often with mocking intent & ``Oh sure, because \textit{that} went well last time.'' \\
\addlinespace
Mixed Signals & Conflicting verbal/contextual cues creating ambiguity & ``I'm fine'' (said with tears) \\
\addlinespace
Strategic Politeness & Surface politeness masking criticism or negative intent & ``That's certainly \textit{one} approach.'' \\
\addlinespace
Passive Aggression & Indirect hostility through apparent compliance & ``No, no, I'll do it myself. Again.'' \\
\addlinespace
Deflection & Avoiding uncomfortable topics through redirection & ``Anyway, how about that weather?'' \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Power Relations}

Each scenario explicitly encodes power dynamics:
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Peer-to-peer} (N=270): Equal status (colleagues, friends, siblings)
    \item \textbf{High-to-low} (N=23): Speaker has authority (boss$\rightarrow$employee, parent$\rightarrow$child)
    \item \textbf{Low-to-high} (N=7): Speaker lacks authority (employee$\rightarrow$boss, student$\rightarrow$teacher)
\end{itemize}

\subsection{Social Domains}

Scenarios span five domains reflecting common sites of pragmatic communication: workplace interactions (e.g., meetings, performance discussions), family relationships (e.g., parent-child, sibling dynamics), social and friendship contexts (e.g., casual gatherings, group outings), service encounters (e.g., customer-staff exchanges), and general contexts that do not fit neatly into the preceding categories. This distribution ensures coverage of settings where indirect speech and emotional inference play distinct social roles.

%==============================================================================
\section{Annotation Process}
\label{sec:annotation}
%==============================================================================

\subsection{Annotator Recruitment and Training}

Fifteen undergraduate students were recruited from an interdisciplinary course at a US liberal arts college, all native English speakers. Annotators received written guidelines covering pragmatic subtypes, Plutchik's emotion categories, and VAD rating procedures (see Appendix~\ref{app:guidelines}), which they reviewed independently before beginning annotation. Each annotator was assigned all 60 scenarios within a single pragmatic subtype, yielding 3 independent annotators per subtype and 15 annotators total across 5 subtypes. Annotation was completed as a graded course assignment over a 12-day period in November--December 2025.

\subsection{Annotation Protocol}

For each scenario, annotators provide:
\begin{enumerate}[leftmargin=*,noitemsep]
    \item \textbf{Primary Emotion:} The speaker's dominant emotion, selected from Plutchik's 8 basic emotions (joy, trust, fear, surprise, sadness, disgust, anger, anticipation)\looseness=-1
    \item \textbf{VAD Ratings:} Valence, Arousal, and Dominance on 7-point scales mapped to $[-1.0, +1.0]$
    \item \textbf{Confidence:} Self-reported annotation confidence on the same scale
\end{enumerate}

Annotations were collected via Label Studio \citep{labelstudio2020} with an estimated completion time of approximately 1 minute per scenario.

\subsection{Quality Control Pipeline}

We developed a 4-level QA pipeline to ensure annotation quality. The first level performs schema validation through automated checks for valid JSON structure, required fields, and enum values. The second level applies statistical consistency checks, including MAD-based outlier detection for annotation timing, straight-lining detection, and self-contradiction identification; this stage flagged 2.6\% of annotations. The third level computes Fleiss' $\kappa$ per subtype with bootstrap confidence intervals (1,000 resamples) to quantify inter-annotator agreement. Finally, all flagged scenarios undergo expert adjudication by a meta-annotator, with 15.7\% of scenarios requiring expert resolution.

%==============================================================================
\section{Dataset Statistics}
\label{sec:stats}
%==============================================================================

\subsection{Overall Distribution}

Table~\ref{tab:distribution} summarizes the dataset composition.

\begin{table}[t]
\caption{CEI Dataset Distribution}
\label{tab:distribution}
\centering
\small
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Attribute} & \textbf{Value} \\
\midrule
Total scenarios & 300 \\
Annotations per scenario & 3 \\
Total annotations & 900 \\
Pragmatic subtypes & 5 \\
Scenarios per subtype & 60 \\
Power relations & 3 \\
Social domains & 5 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Inter-Annotator Agreement}

Table~\ref{tab:agreement} reports Fleiss' $\kappa$ by pragmatic subtype. Because each annotator was assigned to a single subtype (Section~\ref{sec:annotation}), per-subtype $\kappa$ values reflect agreement among the same 3 raters across all 60 scenarios within that subtype. The overall $\kappa$ is computed as a weighted average of per-subtype values. The low overall agreement ($\kappa = 0.12$) is expected given the task: pragmatic inference admits multiple valid interpretations, and even careful annotators frequently disagree on the emotional valence of indirect speech.

\begin{table}[t]
\caption{Inter-Annotator Agreement by Subtype (Fleiss' $\kappa$)}
\label{tab:agreement}
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Subtype} & \textbf{$\kappa$} & \textbf{95\% CI} & \textbf{Interpretation} \\
\midrule
Sarcasm/Irony & 0.25 & [0.17, 0.32] & Fair \\
Strategic Politeness & 0.20 & [0.12, 0.27] & Slight \\
Mixed Signals & 0.16 & [0.09, 0.22] & Slight \\
Deflection & 0.06 & [-0.01, 0.12] & Slight \\
Passive Aggression & -0.06 & [-0.13, -0.00] & Poor \\
\midrule
\textbf{Overall} & \textbf{0.12} & [0.08, 0.16] & Slight \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Human Performance}

We compute human ``accuracy'' as agreement with ground truth (majority vote):

\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Mean annotator agreement:} 57\%
    \item \textbf{Unanimous agreement:} 13\% of scenarios (39/300)
    \item \textbf{Majority agreement:} 46\% of scenarios (138/300)
    \item \textbf{Three-way split:} 41\% of scenarios (123/300)
\end{itemize}

Figure~\ref{fig:difficulty} shows the difficulty distribution overall and by subtype. This validates CEI as a challenging benchmark even for humans.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig2_difficulty_distribution.pdf}
    \caption{Scenario difficulty distribution based on annotator agreement. (a) Overall: 13\% unanimous, 46\% majority, 41\% split. (b) By subtype: sarcasm/irony shows highest agreement; passive aggression shows lowest.}
    \label{fig:difficulty}
\end{figure}

\subsection{Emotion Confusion Patterns}

Figure~\ref{fig:confusion} shows the 8$\times$8 emotion confusion matrix aggregated across all annotations. The most common confusions are:
\begin{itemize}[leftmargin=*,noitemsep]
    \item Sadness $\leftrightarrow$ Surprise (166 pairs)
    \item Anger $\leftrightarrow$ Sadness (132 pairs)
    \item Anger $\leftrightarrow$ Surprise (114 pairs)
\end{itemize}

These confusions are consistent with psychological research on emotion categorization, where negative emotions with similar valence profiles are often difficult to distinguish without additional context.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/fig1_confusion_matrix.pdf}
    \caption{Human annotator confusion matrix for Plutchik's 8 emotions. Values show row-normalized proportions (how often Annotator 1's choice co-occurs with Annotator 2's). Diagonal represents agreement (16.5\% overall). Sadness-surprise and anger-sadness show highest confusion.}
    \label{fig:confusion}
\end{figure}

\subsection{Linguistic Feature Analysis}

We examined whether scenario length predicts annotation difficulty. Figure~\ref{fig:linguistic} shows correlations between text length and annotator agreement. Neither context length ($r=-0.05$) nor utterance length ($r=-0.02$) significantly predicts agreement, suggesting difficulty stems from pragmatic complexity rather than surface features.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig3_linguistic_analysis.pdf}
    \caption{Linguistic features vs. annotator agreement. (a) Context length shows no correlation with agreement ($r=-0.05$). (b) Utterance length similarly uncorrelated ($r=-0.02$). Difficulty stems from pragmatic ambiguity, not text complexity.}
    \label{fig:linguistic}
\end{figure}

%==============================================================================
\section{Evaluation Framework}
\label{sec:eval}
%==============================================================================

For each scenario, models receive the context, speaker/listener roles, and utterance. The task is to predict the speaker's primary emotional state from Plutchik's 8 basic emotions.

We recommend reporting three metrics: accuracy (exact match with ground truth emotion), macro-F1 (F1 averaged across the 8 emotion classes to account for class imbalance), and stratified breakdowns by pragmatic subtype and power relation. This stratification reveals whether models struggle uniformly or exhibit systematic weaknesses on particular pragmatic phenomena. Standardized zero-shot and chain-of-thought prompt templates are provided in the supplementary materials to facilitate reproducible evaluation.

Detailed analysis of model failure patterns and cognitive implications of pragmatic reasoning deficits is left to future work. We provide standardized evaluation scripts and prompt templates to support such investigation. The present paper focuses on dataset construction and validation.

%==============================================================================
\section{Intended Uses and Limitations}
\label{sec:limitations}
%==============================================================================

\subsection{Intended Uses}

CEI is designed to evaluate language models on pragmatic reasoning in social contexts. Appropriate uses include:
\begin{itemize}[leftmargin=*,noitemsep]
    \item Benchmarking pragmatic disambiguation capabilities
    \item Diagnosing failure modes in social AI systems
    \item Training data for emotion inference models (with appropriate caveats)
\end{itemize}

\subsection{Limitations}

\paragraph{Synthetic scenarios.} All scenarios are researcher-authored rather than naturalistic. This enables controlled variation but may not capture the full complexity of real communication.

\paragraph{English only.} The current release covers only English. Pragmatic conventions vary across languages and cultures.

\paragraph{Annotation framing.} The detailed annotation guidelines (Appendix~\ref{app:guidelines}) consistently instruct annotators to label the \textit{speaker's} emotion. However, an introductory overview provided to annotators used ``listener'' framing when describing the task at a high level. Since annotators had the detailed guidelines open during annotation, the speaker-emotion framing likely dominated, but we cannot rule out that some annotators partially followed the overview framing. This discrepancy may contribute to the observed inter-annotator disagreement.

\paragraph{Low agreement is informative, not problematic.} The low inter-annotator agreement ($\kappa = 0.12$) stems from the task itself: pragmatic utterances support multiple valid interpretations. Users should interpret ground truth labels as majority consensus rather than definitive truth, and model disagreement with ground truth may sometimes reflect reasonable alternative readings.

\paragraph{Power relation imbalance.} The distribution of power relations (90\% peer, 8\% high\allowbreak{}$\rightarrow$low, 2\% low$\rightarrow$high) reflects naturalistic communication patterns---most daily interactions occur between equals. However, this limits statistical power for power-stratified analyses. Such evaluation is an important direction for future work, requiring either targeted data collection to increase non-peer scenarios or appropriate small-sample statistical methods.

\paragraph{Modest scale prioritizes quality.} With 300 scenarios (900 annotations), CEI is smaller than crowd-sourced benchmarks like SocialIQa (38K) or SARC (1M). This reflects a deliberate trade-off: expert annotation with multi-annotator validation enables rigorous quality control but limits scale. The 4-level QA pipeline and documented inter-annotator agreement provide transparency that larger datasets often lack.

\subsection{Broader Impact}

CEI could improve AI systems that interpret human communication, with applications in mental health support, conflict mediation, and accessibility. However, misuse could enable surveillance or manipulation. We release under CC-BY-4.0 to enable research while encouraging responsible deployment.

\subsection{Maintenance and Versioning}

CEI follows semantic versioning (v1.0.0). Error corrections will be documented in a changelog. The dataset will be maintained for at least 5 years post-publication. Contact: \texttt{[redacted for blind review]}.

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

We introduced the Contextual Emotional Inference (CEI) Benchmark, a dataset of 300 human-validated scenarios for evaluating pragmatic disambiguation in large language models. CEI combines five pragmatic subtypes, three power relations, and multiple social domains with multi-annotator validation. The observed inter-annotator agreement (Fleiss' $\kappa = 0.12$--$0.25$ by subtype) confirms that pragmatic inference is a task where reasonable people disagree---making it a meaningful challenge for AI systems that must navigate similar uncertainty.

Beyond the dataset itself, our primary contributions are methodological: a documented annotation framework that can be adapted for related tasks, a reusable 4-level QA pipeline for multi-annotator datasets, and a standardized evaluation protocol. CEI is released under CC-BY-4.0 at \url{<anonymous>}.

%==============================================================================
\impact{
CEI enables research on pragmatic reasoning in AI systems, with potential positive impacts on mental health support, accessibility tools, and human-AI communication. Risks include potential misuse for surveillance or manipulation of vulnerable populations. We mitigate these risks through open release (enabling scrutiny), documentation of limitations, and explicit guidance against high-stakes deployment without human oversight.
}

\acks{
[Redacted for blind review]
}

\bibliography{cei-tom_dataset}

%==============================================================================
\appendix
\section{Datasheet for CEI}
\label{app:datasheet}

Following \citet{gebru2021datasheets}, we provide a complete datasheet.

\subsection{Motivation}
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Purpose:} Evaluate pragmatic reasoning in language models
    \item \textbf{Creators:} Academic research team
    \item \textbf{Funding:} University research funds
\end{itemize}

\subsection{Composition}
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Instances:} 300 scenarios with situational context and utterance
    \item \textbf{Labels:} 3 annotations per scenario (emotion + VAD)
    \item \textbf{Splits:} Not predefined; users may create their own
    \item \textbf{Sensitive content:} None; all scenarios are synthetic
\end{itemize}

\subsection{Collection Process}
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Scenarios:} Template-based synthesis with expert curation; 600 initial scenarios were generated, from which 300 were selected after manual review and editing for quality, clarity, and balanced subtype coverage
    \item \textbf{Annotations:} Collected from 15 undergraduate annotators in a university course setting
    \item \textbf{Timeline:} November--December 2025
    \item \textbf{Ethics:} This study was determined to be IRB-exempt (Category 2). Annotation was completed as a graded course assignment; annotators received course credit and were offered co-authorship for high-quality contributions
\end{itemize}

\subsection{Preprocessing}
\begin{itemize}[leftmargin=*,noitemsep]
    \item 4-level QA pipeline applied
    \item 15.7\% of scenarios required expert adjudication
    \item Ground truth established via majority vote + expert override
\end{itemize}

\subsection{Uses}
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Intended:} Research on pragmatic AI evaluation
    \item \textbf{Not recommended:} High-stakes decision-making without human oversight
\end{itemize}

\subsection{Distribution}
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{License:} CC-BY-4.0 (data), MIT (code)
    \item \textbf{Access:} \url{<anonymous>}
\end{itemize}

\subsection{Maintenance}
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Support:} 5 years minimum
    \item \textbf{Contact:} \texttt{[redacted for blind review]}
\end{itemize}

%==============================================================================
\section{Annotation Guidelines}
\label{app:guidelines}

This appendix documents the complete annotation protocol used for CEI.

\subsection{Core Task Definition}

Annotators are asked: \textit{``Given this context and utterance, what emotion does the SPEAKER most likely feel?''} The focus is on the speaker's emotional state as inferred from the pragmatic context, including the situation, roles, and utterance.

\subsection{Emotion Categories}

Annotators select from Plutchik's 8 basic emotions:

\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Joy:} Happiness, contentment, amusement
    \item \textbf{Trust:} Acceptance, confidence in the speaker
    \item \textbf{Fear:} Anxiety, worry, apprehension
    \item \textbf{Surprise:} Unexpected information, confusion
    \item \textbf{Sadness:} Disappointment, hurt, dejection
    \item \textbf{Disgust:} Revulsion, contempt, disapproval
    \item \textbf{Anger:} Frustration, irritation, offense
    \item \textbf{Anticipation:} Expectation, interest, engagement
\end{itemize}

\subsection{VAD Ratings}

For each scenario, annotators provide Valence-Arousal-Dominance ratings on 7-point scales mapped to the range $[-1.0, +1.0]$:

\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Valence} ($-1.0$=very negative, $+1.0$=very positive): How pleasant/unpleasant the speaker feels
    \item \textbf{Arousal} ($-1.0$=very calm, $+1.0$=very activated): The speaker's energy and activation level
    \item \textbf{Dominance} ($-1.0$=very submissive, $+1.0$=very dominant): How in-control the speaker feels
\end{itemize}

\subsection{Subtype Definitions}

\paragraph{Sarcasm/Irony.} The speaker says the opposite of what they mean, often with mocking or critical intent. \textit{Key signal:} Literal interpretation contradicts context.

\paragraph{Mixed Signals.} Verbal content conflicts with contextual cues (tone, situation, body language described). \textit{Key signal:} ``I'm fine'' when clearly not fine.

\paragraph{Strategic Politeness.} Surface-level politeness masks criticism, disagreement, or negative evaluation. \textit{Key signal:} Formally polite language with subtle digs.

\paragraph{Passive Aggression.} Indirect expression of hostility through apparent compliance or withdrawal. \textit{Key signal:} Compliance that feels punishing.

\paragraph{Deflection/Misdirection.} Avoiding uncomfortable topics by changing subject or redirecting attention. \textit{Key signal:} Non-sequitur responses to direct questions.

\subsection{Edge Case Guidance}

\paragraph{Multiple emotions apply.} Select the \textit{primary} or \textit{strongest} emotion. If genuinely equal, choose the more negative emotion (pragmatic ambiguity often signals social friction).

\paragraph{Ambiguous scenarios.} Some scenarios are intentionally ambiguous. Annotate based on your best interpretation; disagreement is expected and informative.

\paragraph{Cultural variation.} Interpret from a general American English perspective. Acknowledge that interpretations may vary across cultures.

\paragraph{Speaker emotion.} Always focus on what the \textit{speaker} most likely feels in the given context, based on the situation, roles, and utterance.

\subsection{Quality Expectations}

\begin{itemize}[leftmargin=*,noitemsep]
    \item Median completion time: 30--60 seconds per scenario
    \item Read full context before responding
    \item Use confidence ratings honestly (low confidence is informative)
    \item Flag scenarios that seem malformed or unclear
\end{itemize}

\end{document}
